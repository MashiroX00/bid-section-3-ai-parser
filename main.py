import os
import json
import re
import asyncio
import time
import pdfplumber
import psycopg2
from psycopg2.extras import Json
from openai import OpenAI
from dotenv import load_dotenv

# ‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°
load_dotenv()

# ================= CONFIGURATION =================
API_KEY = os.getenv("OPENAI_API_KEY")
INPUT_FOLDER = "input_pdfs"
BATCH_FILE_NAME = "batch_input_pg.jsonl"
BATCH_ID_LOG = "current_batch_id.txt"
INTERVAL_TIME = 120 # 2 ‡∏ô‡∏≤‡∏ó‡∏µ
# Database Config
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_NAME = os.getenv("DB_NAME", "postgres")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASS = os.getenv("DB_PASS", "password")

# System Options
SKIP_EXISTING = True       # True = ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏ô DB ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ AI ‡πÉ‡∏´‡∏°‡πà
MODEL_NAME = "gpt-4o-mini" 
CONCURRENT_LIMIT = 20      # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

if not API_KEY:
    raise ValueError("‚ùå Error: OPENAI_API_KEY not found in .env file")

client = OpenAI(api_key=API_KEY)

# ================= PROMPT & SCHEMA =================
TARGET_JSON_SCHEMA = """
{
  "bid_submission_documents_part_1": {
    "1_legal_entity_documents": {
      "case_partnership": { "description": "‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÄ‡∏ä‡πà‡∏ô ‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô", "required_documents": [] },
      "case_company": { "description": "‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÄ‡∏ä‡πà‡∏ô ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≥‡∏Å‡∏±‡∏î", "required_documents": [] }
    },
    "2_individual_documents": { "description": "‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤", "required_documents": [] },
    "3_joint_venture_documents": { "description": "‡∏ú‡∏π‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Ñ‡πâ‡∏≤", "required_documents": [] },
    "4_financial_capability_evidence": { "description": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô", "options": [{"condition": "...", "document": "..."}], "note": "..." },
    "5_general_documents": { "description": "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ", "required_documents": [] }
  }
}
"""

SYSTEM_PROMPT = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ TOR
‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà: ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1 (‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠)" ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
Output: JSON ‡∏ï‡∏≤‡∏° Schema ‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
{TARGET_JSON_SCHEMA}

‡∏Å‡∏é:
- ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà [] ‡∏´‡∏£‡∏∑‡∏≠ null
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏° Key ‡∏≠‡∏∑‡πà‡∏ô‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å Schema
"""

# ================= DATABASE LAYER =================

def get_db_connection():
    return psycopg2.connect(
        host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS
    )

def init_db():
    conn = get_db_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("CREATE SCHEMA IF NOT EXISTS batch_data;")
            cur.execute("""
                CREATE TABLE IF NOT EXISTS batch_data.batch_json(
                    project_id varchar(255) primary key not null,
                    json jsonb,
                    created_at timestamp not null default current_timestamp
                );
            """)
        conn.commit()
    except Exception as e:
        print(f"‚ùå DB Init Error: {e}")
    finally:
        conn.close()

def get_all_existing_ids():
    conn = get_db_connection()
    existing_ids = set()
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT project_id FROM batch_data.batch_json")
            rows = cur.fetchall()
            for row in rows:
                existing_ids.add(row[0])
    finally:
        conn.close()
    return existing_ids

def save_results_to_db(results_list):
    conn = get_db_connection()
    success_count = 0
    try:
        with conn.cursor() as cur:
            query = """
                INSERT INTO batch_data.batch_json (project_id, json, created_at)
                VALUES (%s, %s, CURRENT_TIMESTAMP)
                ON CONFLICT (project_id) 
                DO UPDATE SET 
                    json = EXCLUDED.json, 
                    created_at = CURRENT_TIMESTAMP;
            """
            data_tuples = [(item['id'], Json(item['data'])) for item in results_list]
            cur.executemany(query, data_tuples)
            success_count = len(results_list)
        conn.commit()
    except Exception as e:
        print(f"‚ùå Database Insert Error: {e}")
        conn.rollback()
    finally:
        conn.close()
    return success_count

# ================= TEXT EXTRACTION LAYER =================

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_evidence_section(pdf_path):
    full_text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    full_text += text + "\n"
        
        pattern = r"(‡πì\.\s*‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠.*?)(?=\n\s*‡πì\.‡πí|\n\s*3\.2|\n\s*‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà\s*‡πí)"
        match = re.search(pattern, full_text, re.DOTALL)
        
        if match:
            extracted_content = match.group(1)
            extracted_content = re.sub(r"^‡πì\.\s*‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠", "", extracted_content).strip()
            return clean_text(extracted_content)
        else:
            return None
    except Exception as e:
        return None

# ================= ASYNC PROCESS (STEP 1) =================

async def process_single_file(sem, filename, existing_ids):
    async with sem:
        project_id = os.path.splitext(filename)[0]

        if SKIP_EXISTING and project_id in existing_ids:
            return "SKIPPED"

        file_path = os.path.join(INPUT_FOLDER, filename)
        extracted_text = await asyncio.to_thread(extract_evidence_section, file_path)

        if not extracted_text:
            return "REGEX_FAILED"

        return {
            "custom_id": filename,
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": MODEL_NAME,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:\n{extracted_text}"}
                ],
                "response_format": {"type": "json_object"},
                "temperature": 0.1
            }
        }

async def create_batch_file_async():
    if not os.path.exists(INPUT_FOLDER):
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {INPUT_FOLDER}")
        return None

    pdf_files = [f for f in os.listdir(INPUT_FOLDER) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå PDF")
        return None

    print("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Database...")
    await asyncio.to_thread(init_db)
    existing_ids = await asyncio.to_thread(get_all_existing_ids)
    print(f"üìã ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô DB {len(existing_ids)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

    print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {len(pdf_files)} ‡πÑ‡∏ü‡∏•‡πå (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô {CONCURRENT_LIMIT} threads)...")
    
    sem = asyncio.Semaphore(CONCURRENT_LIMIT)
    tasks = []
    for filename in pdf_files:
        tasks.append(process_single_file(sem, filename, existing_ids))

    results = await asyncio.gather(*tasks)

    valid_tasks = []
    skipped_count = 0
    regex_failed_count = 0

    for res in results:
        if res == "SKIPPED":
            skipped_count += 1
        elif res == "REGEX_FAILED":
            regex_failed_count += 1
        elif isinstance(res, dict):
            valid_tasks.append(res)

    print(f"\n--- ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---")
    print(f"‚è© ‡∏Ç‡πâ‡∏≤‡∏° (‡∏°‡∏µ‡πÉ‡∏ô DB ‡πÅ‡∏•‡πâ‡∏ß): {skipped_count}")
    print(f"‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏° (‡∏´‡∏≤ Section ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠): {regex_failed_count}")
    print(f"‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á (‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà): {len(valid_tasks)}")

    if not valid_tasks:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á")
        return None

    with open(BATCH_FILE_NAME, "w", encoding="utf-8") as f:
        for task in valid_tasks:
            f.write(json.dumps(task, ensure_ascii=False) + "\n")
            
    print(f"üíæ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Batch ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {BATCH_FILE_NAME}")
    return BATCH_FILE_NAME

# ================= SHARED LOGIC (SUBMIT & PROCESS) =================

def upload_and_submit_batch(jsonl_file):
    """‡∏™‡πà‡∏á Batch ‡πÑ‡∏õ OpenAI ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ Batch ID"""
    print("\n‚òÅÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á...")
    try:
        batch_input_file = client.files.create(
            file=open(jsonl_file, "rb"),
            purpose="batch"
        )
        
        batch_job = client.batches.create(
            input_file_id=batch_input_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h"
        )
        
        with open(BATCH_ID_LOG, "w") as f:
            f.write(batch_job.id)
            
        print(f"‚úÖ ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! Batch ID: {batch_job.id}")
        return batch_job.id
        
    except Exception as e:
        print(f"‚ùå Submit Error: {e}")
        return None

def download_and_save_results(batch_id):
    """‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á DB"""
    print(f"‚¨áÔ∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (ID: {batch_id})...")
    try:
        batch_job = client.batches.retrieve(batch_id)
        content = client.files.content(batch_job.output_file_id).text
        
        lines = content.strip().split('\n')
        data_to_save = []
        
        for line in lines:
            try:
                data = json.loads(line)
                filename = data['custom_id']
                project_id = os.path.splitext(filename)[0]
                
                response_body = data['response']['body']
                if 'choices' in response_body:
                    ai_content = response_body['choices'][0]['message']['content']
                    parsed_json = json.loads(ai_content)
                    
                    data_to_save.append({
                        "id": project_id,
                        "data": parsed_json
                    })
            except Exception as e:
                print(f"   ‚ùå Parse Error: {e}")
        
        if data_to_save:
            print(f"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å {len(data_to_save)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏á Database...")
            saved_count = save_results_to_db(data_to_save)
            print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô {saved_count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            return True
    except Exception as e:
        print(f"‚ùå Save Error: {e}")
        return False

# ================= OPTION 3: AUTO PILOT =================

async def run_auto_pilot():
    print("\n" + "="*40)
    print("   üöÄ STARTING AUTO PILOT MODE")
    print("="*40)
    
    # 1. Prepare & Submit
    jsonl_path = await create_batch_file_async()
    if not jsonl_path:
        return

    batch_id = upload_and_submit_batch(jsonl_path)
    if not batch_id:
        return

    # 2. Polling Loop
    print(f"\n--- üîÑ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡πÇ‡∏´‡∏°‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å 2 ‡∏ô‡∏≤‡∏ó‡∏µ) ---")
    start_time = time.time()
    
    while True:
        try:
            batch_job = client.batches.retrieve(batch_id)
            status = batch_job.status
            elapsed = int((time.time() - start_time) / 60)
            
            print(f"‚è±Ô∏è  [{elapsed} ‡∏ô‡∏≤‡∏ó‡∏µ] Status: {status.upper()}")

            if status == "completed":
                print("üéâ ‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß! ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
                download_and_save_results(batch_id)
                print("‚úÖ Auto Pilot ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
                break
            
            elif status in ["failed", "expired", "cancelled"]:
                print(f"‚ùå ‡∏á‡∏≤‡∏ô‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß (Status: {status})")
                if batch_job.errors:
                    print(f"Errors: {batch_job.errors}")
                break
                
            else:
                # validating, in_progress, finalizing
                print(f"üí§ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à... ‡∏£‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô {INTERVAL_TIME // 60} ‡∏ô‡∏≤‡∏ó‡∏µ")
                await asyncio.sleep(INTERVAL_TIME)  # ‡∏£‡∏≠ INTERVAL_TIME ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (2 ‡∏ô‡∏≤‡∏ó‡∏µ)
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error checking status: {e}")
            await asyncio.sleep(60) # ‡∏ñ‡πâ‡∏≤ Error ‡πÉ‡∏´‡πâ‡∏£‡∏≠ 1 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà

# ================= MAIN MENU =================

def main_menu():
    print("\n=========================================")
    print("   TOR PDF EXTRACTOR (PGSQL + BATCH)   ")
    print("=========================================")
    print("1. ‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô (Submit Only) - ‡∏™‡∏£‡πâ‡∏≤‡∏á Batch ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏ö")
    print("2. ‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô (Check & Save) - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ID ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î")
    print("3. ‡∏≠‡∏≠‡πÇ‡∏ï‡πâ (Auto Pilot) - ‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô + ‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à + ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å")
    
    choice = input("\n‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á (1/2/3): ").strip()
    
    if choice == "1":
        jsonl_path = asyncio.run(create_batch_file_async())
        if jsonl_path:
            upload_and_submit_batch(jsonl_path)
            
    elif choice == "2":
        if os.path.exists(BATCH_ID_LOG):
            with open(BATCH_ID_LOG, "r") as f:
                batch_id = f.read().strip()
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡πà‡∏≠‡∏ô
            try:
                job = client.batches.retrieve(batch_id)
                print(f"Status: {job.status}")
                if job.status == "completed":
                    download_and_save_results(batch_id)
                else:
                    print("‚è≥ ‡∏á‡∏≤‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Ñ‡∏£‡∏±‡∏ö")
            except Exception as e:
                print(f"Error: {e}")
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö Batch ID ‡πÄ‡∏î‡∏¥‡∏°")
            
    elif choice == "3":
        asyncio.run(run_auto_pilot())
        
    else:
        print("‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")

if __name__ == "__main__":
    main_menu()